## Project Overview

We aim to focus on protein sequence prediction by researching relevant models and exploring transferable strategies from multimodal approaches. Our goal is to identify and adapt state-of-the-art methods that can enhance the prediction of protein sequences through innovative and efficient modeling techniques.

# Papers-on-Protein

## Protein Generation

### Function to Sequence

**ProtFIM: Fill-in-Middle Protein Sequence Design via Protein Language Models**   
Lee, Youhan, and Hasun Yu  
*ICLR, 2023*  
[[Paper](https://openreview.net/forum?id=9XAZBUfnefS)]

**Equivariant 3D-Conditional Diffusion Models for Molecular Linker Design**  
Ilia Igashov, Hannes Stärk, Clément Vignac, Arne Schneuing, Victor Garcia Satorras, Pascal Frossard, Max Welling, Michael Bronstein and Bruno Correia  
*Nature Machine Intelligence*  
[[Paper](https://www.nature.com/articles/s42256-024-00815-9)] [[Code](https://github.com/igashov/DiffLinker)]  

**Linker-Tuning: Optimizing Continuous Prompts for Heterodimeric Protein Prediction**  
Shuxian Zou, Shentong Mo, Hui Li, Xingyi Cheng, Le Song, Eric Xing  
*NeurIPS, 2023 (submitted)*  
[[Paper](https://openreview.net/pdf?id=g8S53BmXE6)]  
Abstract: This paper introduces Linker-Tuning, a method that adapts ESMFold to predict heterodimer structures efficiently, showing significant improvements over baseline models in accuracy and speed.

**HelixFold-Multimer: Elevating Protein Complex Structure Prediction to New Heights**  
Authors not specified in available content  
*Publication Date: Not Specified*  
[[Paper](https://ar5iv.org/pdf/2404.10260)]  
Abstract: HelixFold-Multimer showcases exceptional accuracy in predicting antigen-antibody complexes, making it a promising tool for advancing drug design and therapeutic development.

**Pairing Interacting Protein Sequences Using Masked Language Modeling**  
Authors not specified in available content  
*Publication Date: Not Specified*  
[[Paper](https://ar5iv.org/pdf/2308.07136)]  
Abstract: This study leverages MSA-based transformers for protein sequence pairing, demonstrating superior performance over traditional coevolution methods, particularly in challenging datasets with low sequence diversity.

**Leveraging Machine Learning Models for Peptide-Protein Interaction Prediction**  
Authors not specified in available content  
*Publication Date: Not Specified*  
[[Paper](https://ar5iv.org/pdf/2310.18249)]  
Abstract: This work integrates machine learning techniques, including SVM and Random Forest models, to predict peptide-protein interactions using sequence-based and structure-based features, enhancing prediction accuracy.

**Reinforcement Learning for Sequence Design Leveraging Protein Language Models**  
Authors not specified in the available content  
*2023*  
[[Paper](https://ar5iv.org/pdf/2407.03154)]  
Abstract: This paper presents a modular approach to leverage existing protein language models within a reinforcement learning framework, focusing on generating protein sequences through mutation policies.

**ReLSO: A Transformer-based Model for Latent Space Optimization and Generation of Proteins**  
Authors not specified in the available content  
*2023*  
[[Paper](https://ar5iv.org/pdf/2201.09948)]  
Abstract: ReLSO integrates sequence and fitness information into a jointly trained autoencoder, optimizing protein sequences by modeling the sequence-function landscape.

**Diffusion Language Models Are Versatile Protein Learners**  
Authors not specified in the available content  
*2023*  
[[Paper](https://ar5iv.org/pdf/2402.18567)]  
Abstract: This work blends diffusion models and language models for protein learning, utilizing discrete diffusion over sequence data for effective modeling of protein structures and interactions.

**Protein Sequence Design with Batch Bayesian Optimisation**  
Authors not specified in the available content  
*2023*  
[[Paper](https://ar5iv.org/pdf/2303.10429)]  
Abstract: The study introduces a Bayesian optimization approach to protein sequence design, focusing on exploring the proximal frontier of the fitness landscape to find high-fitness mutants.

**Network and Sequence-Based Prediction of Protein-Protein Interactions**  
Authors not specified in the available content  
*2023*  
[[Paper](https://ar5iv.org/pdf/2107.03694)]  
Abstract: The paper models protein interactions using sequence similarity and biological indices, predicting interactions based on evolutionary and functional similarities among protein sequences.

**PiFold: Toward effective and efficient protein inverse folding**  
Zhangyang Gao, Cheng Tan, and Stan Z. Li  
*ICLR, 2023*  
[[Paper](https://arxiv.org/abs/2209.12643)] [[Code](https://github.com/A4Bio/PiFold)]  

**Reprogramming Pretrained Language Models for Antibody Sequence Infilling**  
Igor Melnyk, Vijil Chenthamarakshan, Pin-Yu Chen, Payel Das, Amit Dhurandhar, Inkit Padhi, and Devleena Das  
*arXiv Preprint*  
[[Paper](https://arxiv.org/pdf/2210.07144v2)]

**Alphafold Distillation For Improved Inverse Protein Folding**  
Igor Melnyk, Aurelie Lozano, Payel Das, and Vijil Chenthamarakshan  
*arXiv Preprint*  
[[Paper](https://arxiv.org/abs/2210.03488)]

**Protein Design And Variant Prediction Using Autoregressive Generative Models**  
Jung-Eun Shin, Adam Riesselman, Kollasch, Conor McMahon, Elana Simon, Chris Sander, Aashish Manglik, Andrew Kruse, and Debora Marks  
*Nature Communications, 2021*  
[[Paper](https://www.nature.com/articles/s41467-021-22732-w)]

**Protein Sequence Design with a Learned Potential**  
Namrata Anand, Raphael R. Eguchi, Alexander Derry, Russ B. Altman, Po-Ssu Huang.  
*Preprint*  
[[Paper](https://doi.org/10.1101/2020.01.06.895466)]  

**Regression Transformer Enables Concurrent Sequence Regression And Generation For Molecular Language Modelling**  
Jannis Born, and Matteo Manica  
*Nature Machine Intelligence*  
[[Paper](https://www.nature.com/articles/s42256-023-00639-z)] 

**Towards Controllable Protein Design With Conditional Transformers**  
Noelia Ferruz, and Birte Höcker  
*Preprint*  
[[Paper](https://arxiv.org/pdf/2201.07338)]

**Robust Deep Learning Based Protein Sequence Design Using ProteinMPNN**  
J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. M. Wicky, A. Courbet, R. J. de Haas, N. Bethel, P. J. Y. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan, B. Koepnick, H. Nguyen, A. Kang, B. Sankaran, A. K. Bera, N. P. King, and D. Baker  
*Science, 2022*  
[[Paper](https://www.science.org/doi/10.1126/science.add2187)] 

**Linker-Tuning: Optimizing Continuous Prompts for Heterodimeric Protein Prediction**  
Shuxian Zou, Shentong Mo, Hui Li, Xingyi Cheng, Le Song, Eric Xing  
*NeurIPS, 2023 (submitted)*  
[[Paper](https://openreview.net/pdf?id=g8S53BmXE6)]  
Keywords: protein structure prediction, protein language models, parameter-efficient training  
Abstract: This paper introduces Linker-Tuning, a method that adapts ESMFold to predict heterodimer structures efficiently, showing significant improvements over baseline models in accuracy and speed.

**Accurate structure prediction of biomolecular interactions with AlphaFold 3**  
Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J. Ballard, Joshua Bambrick, Sebastian W. Bodenstein, David A. Evans, Chia-Chun Hung, Michael O’Neill, David Reiman, Kathryn Tunyasuvunakool, Zachary Wu, Akvilė Žemgulytė, Eirini Arvaniti, Charles Beattie, Ottavia Bertolli, Alex Bridgland, Alexey Cherepanov, Miles Congreve, Alexander I. Cowen-Rivers, Andrew Cowie, Michael Figurnov, Fabian B. Fuchs, Hannah Gladman, Rishub Jain, Yousuf A. Khan, Caroline M. R. Low, Kuba Perlin, Anna Potapenko, Pascal Savy, Sukhdeep Singh, Adrian Stecula, Ashok Thillaisundaram, Catherine Tong, Sergei Yakneen, Ellen D. Zhong, Michal Zielinski, Augustin Žídek, Victor Bapst, Pushmeet Kohli, Max Jaderberg, Demis Hassabis & John M. Jumper  
*Nature*  
**Keywords:** Diffusion-based architecture, Protein structure modelling, Biomolecular space modelling  
This paper introduces AlphaFold 3, which uses a diffusion-based architecture to accurately predict biomolecular interactions and protein structures.  
[[Paper](https://www.nature.com/articles/s41586-023-06415-8)]

**A backbone-centred energy function of neural networks for protein design**  
B Huang, Y Xu, X Hu, Y Liu, S Liao, J Zhang, C Huang  
*Nature*  
**Keywords:** Energy function, MD simulation, Backbone-centred  
The study presents a backbone-centred energy function that integrates neural networks and MD simulations for efficient protein design.  
[[Paper](https://www.nature.com/articles/s41586-023-06415-8)]

**De novo protein design by deep network hallucination**  
Ivan Anishchenko, Samuel J. Pellock, Tamuka M. Chidyausiku, Theresa A. Ramelot, Sergey Ovchinnikov, Jingzhou Hao, Khushboo Bafna, Christoffer Norn, Alex Kang, Asim K. Bera, Frank DiMaio, Lauren Carter, Cameron M. Chow, Gaetano T. Montelione & David Baker  
*Nature*  
**Keywords:** Hallucination, Inpainting, Protein design  
The paper explores a novel approach for de novo protein design using deep network hallucination and inpainting techniques.  
[[Paper](https://www.nature.com/articles/s41586-023-06415-8)]

**Design of protein-binding proteins from the target structure alone**  
Longxing Cao, Brian Coventry, Inna Goreshnik, Buwei Huang, William Sheffler, Joon Sung Park, Kevin M. Jude, Iva Marković, Rameshwar U. Kadam, Koen H. G. Verschueren, Kenneth Verstraete, Scott Thomas Russell Walsh, Nathaniel Bennett, Ashish Phal, Aerin Yang, Lisa Kozodoy, Michelle DeWitt, Lora Picton, Lauren Miller, Eva-Maria Strauch, Nicholas D. DeBouver, Allison Pires, Asim K. Bera, Samer Halabiya, Bradley Hammerson, Wei Yang, Steffen Bernard, Lance Stewart, Ian A. Wilson, Hannele Ruohola-Baker, Joseph Schlessinger, Sangwon Lee, Savvas N. Savvides, K. Christopher Garcia & David Baker  
*Nature*  
**Keywords:** Binding site  
This research focuses on designing protein-binding proteins using only the target structure, enhancing binding affinity and specificity.  
[[Paper](https://www.nature.com/articles/s41586-023-06415-8)]

**Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations**  
Payel Das, Tom Sercu, Kahini Wadhawan, Inkit Padhi, Sebastian Gehrmann, Flaviu Cipcigan, Vijil Chenthamarakshan, Hendrik Strobelt, Cicero dos Santos, Pin-Yu Chen, Yi Yan Yang, Jeremy P. K. Tan, James Hedrick, Jason Crain & Aleksandra Mojsilovic  
*Nature Biomedical Engineering*  
**Keywords:** Antimicrobials, Generative autoencoder, Molecular dynamics  
The paper discusses a method for accelerated antimicrobial discovery using deep generative models coupled with molecular dynamics simulations.  
[[Paper](https://www.nature.com/articles/s42256-023-00639-z)]

**Discovering de novo peptide substrates for enzymes using machine learning**  
Lorillee Tallorin, JiaLei Wang, Woojoo E. Kim, Swagat Sahu, Nicolas M. Kosa, Pu Yang, Matthew Thompson, Michael K. Gilson, Peter I. Frazier, Michael D. Burkart & Nathan C. Gianneschi  
*Nature Communications*  
**Keywords:** Enzymes design, Machine learning  
This study leverages machine learning to discover de novo peptide substrates for enzyme design, improving enzyme efficiency.  
[[Paper](https://www.nature.com/articles/s41467-021-22732-w)]

**ECNet is an evolutionary context-integrated deep learning framework for protein engineering**  
Yunan Luo, Guangde Jiang, Tianhao Yu, Yang Liu, Lam Vo, Hantian Ding, Yufeng Su, Wesley Wei Qian, Huimin Zhao & Jian Peng  
*Nature Communications*  
**Keywords:** Functional fitness, Evolutionary  
The research presents ECNet, a deep learning framework integrating evolutionary context for improved protein engineering and functional fitness prediction.  
[[Paper](https://www.nature.com/articles/s41467-021-22732-w)]

**Deep generative models create new and diverse protein structures**  
Zeming Lin, Tom Sercu, Yann LeCun  
*ICML*  
**Keywords:** Diversity, Generative model, Protein design  
This research demonstrates the use of deep generative models to create diverse and novel protein structures, enhancing the potential for new protein functionalities.  
[[Paper](https://www.nature.com/articles/s42256-023-00639-z)]

**Protein generation with evolutionary diffusion: sequence is all you need**  
Sarah Alamdari, Nitya Thakkar, Rianne van den Berg, Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, Kevin K Yang  
*Arxiv*  
**Keywords:** Diffusion model, Deep generative model, Protein generation, Framework, Sequence design  
The study explores the application of evolutionary diffusion models in protein generation, emphasizing sequence design.  
[[Paper](https://arxiv.org/pdf/2301.13154)]

**A high-level programming language for generative protein design**  
Brian Hie, Salvatore Candido, Zeming Lin, Ori Kabeli, Roshan Rao, Nikita Smetanin, Tom Sercu, Alexander Rives  
*Arxiv*  
**Keywords:** ESMFold, Language model, Energy-based  
The paper introduces a high-level programming language tailored for generative protein design, leveraging ESMFold and energy-based models for efficient design.  
[[Paper](https://arxiv.org/pdf/2301.13154)]

**ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing**  
Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, Yonghong Tian  
*Arxiv*    
[[Paper](https://arxiv.org/pdf/2402.16445)]

### Function to Structure

**Protein Sequence and Structure Co-Design with Equivariant Translation**   
Chence Shi, Chuanrui Wang, Jiarui Lu, Bozitao Zhong, and Jian Tang  
*ICLR, 2023*  
[[Paper](https://arxiv.org/abs/2210.08761)] [[Code](https://github.com/shichence/ProtSeed)]  

**Protein Sequence and Structure Co-Design with Equivariant Translation**   
Nathan C. Frey, Daniel Berenberg, Karina Zadorozhny, Joseph Kleinhenz, Julien Lafrance-Vanasse, Isidro Hotzel, Yan Wu, Stephen Ra, Richard Bonneau, Kyunghyun Cho, Andreas Loukas, Vladimir Gligorijevic, and Saeed Saremi  
*ICLR, 2024*  
[[Paper](https://arxiv.org/pdf/2306.12360)]

**De novo design of protein structure and function with RFdiffusion**   
Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh, Isaac Sappington, Susana Vázquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Sergey Ovchinnikov, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek and David Baker  
*Nature, 2023*  
[[Paper](https://www.nature.com/articles/s41586-023-06415-8)] [[Code](https://github.com/RosettaCommons/RFdiffusion)]  

**Scaffolding protein functional sites using deep learning**  
Jue Wang, Sidney Lisanza, David Juergens, Doug Tischer, Joseph L. Watson, Karla M. Castro, Robert Ragotte, Amijai Saragovi, Lukas F. Milles, Minkyung Baek, Ivan Anishchenko, Wei Yang, Derrick R. Hicks, Marc Expòsit, Thomas Schlichthaerle, Jung-Ho Chun, Justas Dauparas, Nathaniel Bennett, Basile I. M. Wicky, Andrew Muenks, Frank DiMaio, Bruno Correia, Sergey Ovchinnikov, David Baker  
*Science*  
**Keywords:** Functional site, Deep learning, Hallucination, Inpainting  
This paper highlights a deep learning approach for scaffolding protein functional sites, incorporating hallucination and inpainting techniques to enhance functionality.  
[[Paper](https://www.science.org/doi/10.1126/science.add2187)]

**Conditional Antibody Design as 3D Equivariant Graph Translation**  
Xiangzhe Kong, Wenbing Huang, Yang Liu  
*ICML*  
**Keywords:** Antibody design, Graph translation  
The study focuses on conditional antibody design using 3D equivariant graph translation to improve antibody binding and specificity.  
[[Paper](https://arxiv.org/pdf/2306.12360)]

**Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion generative models**  
Joseph L. Watson, David Juergens, Nathaniel R. Bennett, Brian L. Trippe, Jason Yim, Helen E. Eisenach, Woody Ahern, Andrew J. Borst, Robert J. Ragotte, Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel, Samuel J. Pellock, Alexis Courbet, William Sheffler, Jue Wang, Preetham Venkatesh, Isaac Sappington, Susana Vázquez Torres, Anna Lauko, Valentin De Bortoli, Emile Mathieu, Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio, Minkyung Baek, David Baker  
*Arxiv*  
**Keywords:** Diffusion, General deep learning framework, De novo binder design  
The research integrates structure prediction networks with diffusion generative models for accurate and broadly applicable protein design.  
[[Paper](https://www.nature.com/articles/s41586-023-06415-8)]

**Function-guided protein design by deep manifold sampling**  
Vladimir Gligorijević, Daniel Berenberg, Stephen Ra, Simon Kelow, Kyunghyun Cho  
*Arxiv*  
**Keywords:** Sequence denoising autoencoder, Deep manifold sampling  
This paper presents a function-guided approach to protein design using deep manifold sampling and sequence denoising autoencoders.  
[[Paper](https://arxiv.org/pdf/2306.12360)]

**Deep sharpening of topological features for de novo protein design**  
Zander Harteveld, Joshua Southern, Michaël Defferrard, Andreas Loukas, Pierre Vandergheynst, Micheal Bronstein, Bruno Correia  
*ICML*  
**Keywords:** Variational autoencoder, Topological features, Sharpen  
[[Paper](https://arxiv.org/pdf/2306.12360)]

## Protein Representation Learning

**Protein Representation Learning via Knowledge Enhanced Primary Structure Modeling**  
Hong-Yu Zhou, Yunxiang Fu, Zhicheng Zhang, Cheng Bian, and Yizhou Yu  
*ICLR, 2023*  
[[Paper](https://arxiv.org/abs/2301.13154)]  

**Protein Representation Learning By Geometric Structure Pretraining**  
Jiaxin Xie, Hao Ouyang, Jingtan Piao, Chenyang Lei, Qifeng Chen  
*ICLR, 2023*  
[[Paper](https://arxiv.org/abs/2203.06125)]  

**Multi-Level Protein Structure Pre-Training With Prompt Learning**  
Titas Anciukevicius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan Bilen, Niloy J. Mitra, Paul Guerrero  
*ICLR, 2023*  
[[Paper](https://openreview.net/forum?id=XGagtiJ8XC)]

**Protein Representation Learning by Geometric Structure Pretraining**  
Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, Jian Tang  
*Arxiv*  
**Keywords:** Drug discovery, Drug design, Generative models of new molecular structures  
The study proposes a geometric structure pretraining approach for protein representation learning, aimed at improving drug discovery and design.  
[[Paper](https://arxiv.org/abs/2203.06125)]

**Language models generalize beyond natural proteins**  
Robert Verkuil, Ori Kabeli, Yilun Du, Basile I. M. Wicky, Lukas F. Milles, Justas Dauparas, David Baker, Sergey Ovchinnikov, Tom Sercu, Alexander Rives  
*Arxiv*  
**Keywords:** ESMFold, Language model, Fixed backbone design  
This research shows how language models can generalize beyond natural proteins, offering new insights into protein structure prediction and design.  
[[Paper](https://arxiv.org/abs/2306.12360)]

## Protein Understanding

**LucaOne: Generalized Biological Foundation Model with Unified Nucleic Acid and Protein Language**  
Yong He, Pan Fang, Yongtao Shan, Yuanfei Pan, Yanhong Wei, Yichang Chen, Yihao Chen, Yi Liu, Zhenyu Zeng, Zhan Zhou, Feng Zhu, Edward C. Holmes, Jieping Ye, Jun Li, Yuelong Shu, Mang Shi, and Zhaorong Li  
*ArXiv*  
[[Paper](https://www.biorxiv.org/content/10.1101/2024.05.10.592927v1)] [[Code](https://github.com/LucaOne/LucaOne?tab=readme-ov-file)]

**SaProt: Protein Language Modeling with Structure-aware Vocabulary**  
Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, and Fajie Yuan  
*ArXiv*  
[[Paper](https://www.biorxiv.org/content/10.1101/2023.10.01.560349v1)] [[Code](https://github.com/westlake-repl/SaProt)]

**BioMedGPT: Open Multimodal Generative Pre-trained Transformer for BioMedicine**  
Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie  
*ArXiv*  
[[Paper](https://arxiv.org/pdf/2308.09442)] [[Code](https://github.com/PharMolix/OpenBioMed)]

**PQA: Zero-shot Protein Question Answering for Free-form Scientific Enquiry with Large Language Models**  
Eli M Carrami, Sahand Sharifzadeh  
*ArXiv*  
[[Paper](https://arxiv.org/pdf/2402.13653)] [[Code](https://github.com/EMCarrami/Pika)]

**InstructProtein: Aligning Human and Protein Language via Knowledge Instruction**  
Zeyuan Wang, Qiang Zhang, Keyan Ding, Ming Qin, Xiang Zhuang, Xiaotong Li, and Huajun Chen  
*ArXiv*  
[[Paper](https://arxiv.org/pdf/2310.03269)] [[Code](https://github.com/HICAI-ZJU/InstructProtein)]

**ProtT3: Protein-to-Text Generation for Text-based Protein Understanding**  
Zhiyuan Liu, An Zhang, Hao Fei, Enzhi Zhang, Xiang Wang, Kenji Kawaguchi, and Tat-Seng Chua  
*ArXiv*  
[[Paper](https://arxiv.org/pdf/2405.12564)] [[Code](https://github.com/acharkq/ProtT3)]

**Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains**  
Junhong Shen, Neil Tenenholtz, James Brian Hall, David Alvarez-Melis, and Nicolo Fusi  
*ArXiv*  
[[Paper](https://arxiv.org/pdf/2402.05140)] [[Code](https://github.com/sjunhongshen/Tag-LLM)]

## Benchmark

**On Pre-Trained Language Models For Antibody**  
Danqing Wang, Fei Ye, and Hao Zhou  
*arXiv Preprint*  
[[Paper](https://arxiv.org/abs/2301.12112)]  

**PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding**  
Minghao Xu, Zuobai Zhang, Jiarui Lu, Zhaocheng Zhu, Yangtian Zhang, Chang Ma, Runcheng Liu, and Jian Tang  
*NeurIPS, 2022*  
[[Paper](https://arxiv.org/abs/2206.02096)]  [[Project Page](https://torchprotein.ai/benchmark)]  

## Multimodal Transfer Reference
**AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling**  
Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yugang Jiang, Xipeng Qiu  
*Fudan University, Multimodal Art Projection Research Community, Shanghai AI Laboratory*  
[[Paper](https://junzhan2000.github.io/AnyGPT.github.io)]  
Abstract: AnyGPT introduces a unified framework that integrates multimodal data into a single language model through discrete sequence modeling, facilitating seamless understanding and generation across various modalities.

**SHOW-O: One Single Transformer to Unify Multimodal Understanding and Generation**  
Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, Mike Zheng Shou  
*Show Lab, National University of Singapore; ByteDance*  
Abstract: SHOW-O presents a unified Transformer architecture that integrates multimodal understanding and generation, enabling efficient and high-quality performance across diverse tasks involving visual and textual data.

**Semantic Alignment for Multimodal Large Language Models**  
Tao Wu, Mengze Li, Jingyuan Chen, Wei Ji, Wang Lin, Jinyang Gao, Kun Kuang, Zhou Zhao, Fei Wu  
*Zhejiang University, National University of Singapore, Alibaba Group*  
Abstract: This work introduces a novel approach for aligning semantics in multimodal large language models, enhancing the coherence and consistency of information across different modalities to improve understanding and generation tasks.

## Dataset Construction
**Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models**  
Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Huajun Chen  
*ICLR 2024*  
[[Paper](https://doi.org/10.48550/arXiv.2306.08018)]  

**A Fine-tuning Dataset and Benchmark for Large Language Models for Protein Understanding**  
Yiqing Shen, Zan Chen, Michail Mamalakis, Luhan He, Haiyang Xia, Tianbin Li, Yanzhou Su, Junjun He, Yu Guang Wang  
*Submitted on 8 Jun 2024, last revised 8 Jul 2024*  
[[Paper](https://doi.org/10.48550/arXiv.2406.05540)]  






